import streamlit as st
import os 
import json
import firebase_admin
from firebase_admin import credentials, firestore
from sentence_transformers import SentenceTransformer
import anthropic
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import base64
from streamlit_agraph import agraph, Node, Edge, Config # ğŸ‘ˆ ã‚°ãƒ©ãƒ•ç”¨

# --- 1. Firestoreæ¥ç¶š ---
@st.cache_resource
def setup_firestore():
    if not firebase_admin._apps:
        try:
            cert_json_string = st.secrets["firebase"]["cert_json"] 
            cert_dict = json.loads(cert_json_string) 
            cred = credentials.Certificate(cert_dict)
            firebase_admin.initialize_app(cred)
        except Exception as e:
            st.error(f"Firestoreæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
            return None
    return firestore.client()

# --- 2. RAGæ¤œç´¢ãƒ­ã‚¸ãƒƒã‚¯ ---
@st.cache_resource
def load_embedding_model():
    return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

def run_rag_search(query, selected_categories):
    db = setup_firestore()
    if not db: return {"answer": "DBæ¥ç¶šå¤±æ•—", "sources": [], "context": "", "meta_context": ""}
    
    model = load_embedding_model()
    
    try:
        query_embedding = model.encode(query)
        
        all_docs = []
        docs_stream = db.collection("tech_docs").stream()
        
        for doc in docs_stream:
            data = doc.to_dict()
            data['doc_id'] = doc.id
            if data.get('category') in selected_categories:
                all_docs.append(data)

        if not all_docs:
            return {"answer": "ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚", "sources": [], "context": "", "meta_context": ""}

        doc_embeddings = np.array([doc['embedding'] for doc in all_docs])
        similarities = cosine_similarity(query_embedding.reshape(1, -1), doc_embeddings).flatten()
        
        top_indices = np.argsort(similarities)[::-1][:5]
        top_docs = [all_docs[i] for i in top_indices]

        context_text = "\n\n---\n\n".join([doc.get('content', '') for doc in top_docs])
        
        # ğŸš¨ æ€è€ƒæ·±æ˜ã‚Šç”¨ã®ã€Œè¦ç´„ãƒ»åˆ†æã€ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        meta_context_list = []
        for doc in top_docs:
            title = doc.get('title', 'No Title')
            summary = doc.get('summary_section', '(è¦ç´„ãªã—)')
            analysis = doc.get('analysis_section', '(åˆ†æãªã—)')
            meta_context_list.append(f"â– äº‹ä¾‹: {title}\n[è¦ç´„]\n{summary}\n[åˆ†æ]\n{analysis}")
        meta_context = "\n\n".join(meta_context_list)
        
        client = anthropic.Anthropic(api_key=st.secrets["CLAUDE_API_KEY"])
        
        prompt = f"""
        ã‚ãªãŸã¯å®¶æ—å‘ã‘æŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰ç›¸è«‡ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®æŠ€è¡“æƒ…å ±ã‚’å‚è€ƒã«ã€è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚
        ã€æŠ€è¡“æƒ…å ±ã€‘
        {context_text}
        ã€è³ªå•ã€‘
        {query}

        ã€å›ç­”å½¢å¼ã€‘
        - ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ã
        - å¿…ãšå…·ä½“çš„ãªæŠ€è¡“åã¨å‡ºå…¸ï¼ˆæ–‡æ›¸ã‚¿ã‚¤ãƒˆãƒ«ï¼‰ã‚’æŒ™ã’ã‚‹
        """
        
        response = client.messages.create(
            model="claude-3-haiku-20240307",
            max_tokens=2000,
            messages=[{"role": "user", "content": prompt}]
        )
        
        sources = [doc.get('title', 'ä¸æ˜') for doc in top_docs]
        
        return {
            "answer": response.content[0].text,
            "sources": sources,
            "context": context_text,
            "meta_context": meta_context # ğŸ‘ˆ ã“ã‚Œã‚’æ·±æ˜ã‚Šæ©Ÿèƒ½ã«æ¸¡ã—ã¾ã™
        }
            
    except Exception as e:
        return {"answer": f"ã‚¨ãƒ©ãƒ¼: {e}", "sources": [], "context": "", "meta_context": ""}

# --- ç”ŸæˆAIå…±é€šé–¢æ•° ---
def call_claude_json(prompt):
    client = anthropic.Anthropic(api_key=st.secrets["CLAUDE_API_KEY"])
    try:
        response = client.messages.create(
            model="claude-3-haiku-20240307",
            max_tokens=1000,
            messages=[{"role": "user", "content": prompt}]
        )
        content = response.content[0].text
        s_idx = content.find("{")
        e_idx = content.rfind("}")
        if s_idx != -1 and e_idx != -1:
            json_str = content[s_idx:e_idx+1]
            return json.loads(json_str, strict=False)
        else:
            return None
    except Exception as e:
        st.error(f"AIç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None

# --- Mermaidå›³ã®æç”»é–¢æ•° (ç”»åƒå¤‰æ›ç‰ˆ: æœ€ã‚‚å®‰å®š) ---
def render_mermaid(graph_code):
    graphbytes = graph_code.encode("utf8")
    base64_bytes = base64.urlsafe_b64encode(graphbytes)
    base64_string = base64_bytes.decode("ascii")
    url = f"https://mermaid.ink/img/{base64_string}"
    st.image(url, use_container_width=True)

# --- æ–°æ©Ÿèƒ½ç¾¤ ---
def generate_future_career(topic):
    prompt = f"""
    ã‚ãªãŸã¯2035å¹´ã®ã‚­ãƒ£ãƒªã‚¢ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚
    ãƒˆãƒ”ãƒƒã‚¯: '{topic}' ã«åŸºã¥ã„ã¦ã€æœªæ¥çš„ã§ã‹ã£ã“ã„ã„æ¶ç©ºã®è·æ¥­ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
    ã€é‡è¦ã€‘æ—¥æœ¬èªã§å‡ºåŠ›ã€‚
    Output format (JSON):
    {{
        "job_title": "è‹±èªå / æ—¥æœ¬èªå",
        "estimated_salary": "15,000,000 JPY",
        "required_skills": ["ã‚¹ã‚­ãƒ«1", "ã‚¹ã‚­ãƒ«2", "ã‚¹ã‚­ãƒ«3"],
        "mission": "ãƒŸãƒƒã‚·ãƒ§ãƒ³"
    }}
    Only output the JSON.
    """
    return call_claude_json(prompt)

def generate_future_diary(topic):
    prompt = f"""
    ã‚ãªãŸã¯å°èª¬å®¶ã§ã™ã€‚2035å¹´ã‚’èˆå°ã«ã€'{topic}' ãŒæ—¥å¸¸ã«ãªã£ãŸä¸–ç•Œã®ã‚·ãƒ§ãƒ¼ãƒˆã‚·ãƒ§ãƒ¼ãƒˆæ—¥è¨˜ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚
    ã€é‡è¦ã€‘æ—¥æœ¬èªã§å‡ºåŠ›ã€‚
    Output format (JSON):
    {{
        "date": "2035å¹´XæœˆXæ—¥ (å¤©æ°—)",
        "title": "ã‚¿ã‚¤ãƒˆãƒ«",
        "author_profile": "å±æ€§",
        "content": "æœ¬æ–‡..."
    }}
    Only output the JSON.
    """
    return call_claude_json(prompt)

# ğŸš¨ ä¿®æ­£: æ€è€ƒã®æ·±æ˜ã‚Š (DBãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãåˆ†æ)
def generate_thought_expansion(topic, mode, meta_context=""):
    base_inst = ""
    if meta_context:
        base_inst = f"ä»¥ä¸‹ã¯æ¤œç´¢ã•ã‚ŒãŸæŠ€è¡“è³‡æ–™ã®è¦ç´„ã¨åˆ†æãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚ã“ã‚Œã«åŸºã¥ãåˆ†æã—ã¦ãã ã•ã„ã€‚\nã€å‚ç…§ãƒ‡ãƒ¼ã‚¿ã€‘\n{meta_context}"
    
    instructions = {
        "abstract": "å‚ç…§ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€å…±é€šã™ã‚‹æˆåŠŸæ³•å‰‡ã‚„æ§‹é€ çš„ãªå¼·ã¿ã‚’æŠ½å‡ºã—ã€ä¸Šä½æ¦‚å¿µã‚’è¨€èªåŒ–ã—ã¦ãã ã•ã„ã€‚",
        "concrete": "å‚ç…§ãƒ‡ãƒ¼ã‚¿ã®å¼·ã¿ã‚’æ´»ã‹ã—ã¦ã€2030å¹´ã®ç¤¾ä¼šèª²é¡Œè§£æ±ºã«å‘ã‘ãŸå…·ä½“çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚",
        "analogous": "å‚ç…§ãƒ‡ãƒ¼ã‚¿ã®æˆåŠŸãƒ­ã‚¸ãƒƒã‚¯ã‚’ã€å…¨ãç•°ãªã‚‹ç•°åˆ†é‡ã«è»¢ç”¨ã™ã‚‹ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’å‡ºã—ã¦ãã ã•ã„ã€‚"
    }
    titles = {
        "abstract": "â¬†ï¸ æŠ½è±¡åŒ–ï¼šå¼·ã¿ã®æ³•å‰‡åŒ–",
        "concrete": "â¬‡ï¸ å…·ä½“åŒ–ï¼šç¤¾ä¼šå®Ÿè£…ãƒ—ãƒ©ãƒ³",
        "analogous": "â†”ï¸ æ¨ªå±•é–‹ï¼šç•°åˆ†é‡ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³"
    }
    
    prompt = f"""
    ã‚ãªãŸã¯æŠ€è¡“ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ã‚¹ãƒˆã§ã™ã€‚ãƒˆãƒ”ãƒƒã‚¯: '{topic}' ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚
    {base_inst}
    æŒ‡ç¤º: {instructions.get(mode, "")}
    ã€é‡è¦ã€‘æ—¥æœ¬èªã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
    Output format (JSON):
    {{
        "title": "{titles.get(mode, 'åˆ†æçµæœ')}",
        "items": ["æ´å¯Ÿ1", "æ´å¯Ÿ2", "æ´å¯Ÿ3", "æ´å¯Ÿ4", "æ´å¯Ÿ5"]
    }}
    Only output the JSON.
    """
    return call_claude_json(prompt)

def generate_tech_hierarchy(topic):
    client = anthropic.Anthropic(api_key=st.secrets["CLAUDE_API_KEY"])
    prompt = f"""
    Create a hierarchical technology map for: '{topic}'.
    Extract main keywords. Labels MUST be in Japanese.
    Output ONLY valid Graphviz DOT code.
    """
    try:
        response = client.messages.create(
            model="claude-3-haiku-20240307",
            max_tokens=1000,
            messages=[{"role": "user", "content": prompt}]
        )
        dot_code = response.content[0].text
        return dot_code.replace("```graphviz", "").replace("```", "").strip()
    except:
        return None

# --- ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•æ§‹ç¯‰é–¢æ•° ---
@st.cache_data(ttl=3600)
def build_knowledge_graph(target_categories):
    db = setup_firestore()
    if not db: return [], []
    
    nodes = []
    edges = []
    existing_nodes = set()
    
    docs = db.collection("tech_docs").select(['title', 'category', 'tags']).stream()
    
    for doc in docs:
        d = doc.to_dict()
        category = d.get('category', 'General')
        if category not in target_categories:
            continue
            
        doc_id = d.get('title', 'No Title')
        tags = d.get('tags', [])
        
        if doc_id not in existing_nodes:
            nodes.append(Node(id=doc_id, label=doc_id, size=15, color="#4F8BF9", shape="dot"))
            existing_nodes.add(doc_id)
        
        for tag in tags:
            tag_id = f"tag_{tag}"
            if tag_id not in existing_nodes:
                nodes.append(Node(id=tag_id, label=tag, size=10, color="#FF6B6B", shape="diamond"))
                existing_nodes.add(tag_id)
            edges.append(Edge(source=doc_id, target=tag_id, color="#DDDDDD"))
            
    return nodes, edges

@st.cache_data(ttl=600)
def get_all_data_as_df():
    db = setup_firestore()
    if not db: return pd.DataFrame()
    docs_list = []
    for doc in db.collection("tech_docs").stream():
        d = doc.to_dict()
        docs_list.append({"Title": d.get('title', ''), "Category": d.get('category', '')})
    return pd.DataFrame(docs_list)

# --- 4. èªè¨¼ãƒ­ã‚¸ãƒƒã‚¯ ---
def check_password():
    input_pass = st.session_state.get("password_input")
    authorized_users = st.secrets.get("user_passwords", {})
    for username, password in authorized_users.items():
        if input_pass == password:
            del st.session_state["password_input"]
            st.session_state["current_user"] = username
            return True
    if input_pass == st.secrets.get("APP_PASSWORD"):
        del st.session_state["password_input"]
        st.session_state["current_user"] = "Family Member"
        return True
    return False

if "password_correct" not in st.session_state:
    st.session_state["password_correct"] = False
if "current_user" not in st.session_state:
    st.session_state["current_user"] = "Guest"

if not st.session_state["password_correct"]:
    st.title("âš”ï¸ CAREER DATA VAULT: AUTH")
    st.markdown("##### æ¬¡ä¸–ä»£æˆ¦ç•¥AIã¸ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã«ã¯ã€èªè¨¼ãŒå¿…è¦ã§ã™ã€‚")
    with st.form("login_form"):
        st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰", type="password", key="password_input")
        if st.form_submit_button("Login"):
            if check_password():
                st.session_state["password_correct"] = True
                st.rerun() 
            else:
                st.error('ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒé–“é•ã£ã¦ã„ã¾ã™ã€‚')
    st.stop() 

# --- 5. ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªç”»é¢ ---

st.sidebar.title("ğŸ”§ Control Panel")
user_name = st.session_state.get("current_user", "Guest")
st.sidebar.caption(f"Login as: **{user_name}**")

if st.sidebar.button("ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ", key='logout_top'):
    st.session_state["password_correct"] = False
    st.session_state["current_user"] = None
    st.session_state.rag_result = None
    st.rerun()

#app_mode = st.sidebar.radio("ãƒ¢ãƒ¼ãƒ‰é¸æŠ", ["ğŸ’¬ AIãƒãƒ£ãƒƒãƒˆ (RAG)", "ğŸ“š ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°ä¸€è¦§", "ğŸ•¸ï¸ ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•"])
app_mode = st.sidebar.radio(
    "ãƒ¢ãƒ¼ãƒ‰é¸æŠ", 
    ["ğŸ’¬ AIãƒãƒ£ãƒƒãƒˆ (RAG)", "ğŸ“š ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°ä¸€è¦§", "ğŸ•¸ï¸ ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•", "ğŸ“– ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦"]
)
CATEGORY_MAPPING = {
    "Gartner Hype Cycle 2025": "gartner_2025",
    "æ—¥çµŒBP æŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰": "nikkei_bp_2025_2035",
    "æ¬¡ä¸–ä»£ç™ºé›»æŠ€è¡“": "æ¬¡ä¸–ä»£ç™ºé›»",
    "è‡ªå‹•è»Šç”£æ¥­äºˆæ¸¬ 2045": "è‡ªå‹•è»Šç”£æ¥­2045",
    "Articles: AI Info": "AIinfo",
    "Articles: Python & Web": "python_and_webtech",
    "Articles: Quality & Security": "Quality_and_Sequrity",
    "Articles: Semiconductor": "Semiconductor",
    "Articles: Tips": "Tips"
}

st.sidebar.markdown("---")

selected_categories = []
if app_mode == "ğŸ’¬ AIãƒãƒ£ãƒƒãƒˆ (RAG)":
    st.sidebar.subheader("ğŸ” RAGæ¤œç´¢å¯¾è±¡ã‚½ãƒ¼ã‚¹")
    st.sidebar.caption("æ¤œç´¢ç¯„å›²ã‚’é¸æŠã—ã¦ãã ã•ã„")
    for label, category_id in CATEGORY_MAPPING.items():
        if st.sidebar.checkbox(label, value=True, key=f"rag_check_{category_id}"):
            selected_categories.append(category_id)

elif app_mode == "ğŸ•¸ï¸ ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•":
    st.sidebar.subheader("ğŸ•¸ï¸ ã‚°ãƒ©ãƒ•è¡¨ç¤ºå¯¾è±¡ã‚½ãƒ¼ã‚¹")
    st.sidebar.caption("å¯è¦–åŒ–ã—ãŸã„ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠã—ã¦ãã ã•ã„")
    for label, category_id in CATEGORY_MAPPING.items():
        if st.sidebar.checkbox(label, value=False, key=f"graph_check_{category_id}"):
            selected_categories.append(category_id)
elif app_mode == "ğŸ“– ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦":
    st.title("ğŸ“– About This Project")
    
    # README.md ã‚’èª­ã¿è¾¼ã‚“ã§è¡¨ç¤º
    try:
        with open("README.md", "r", encoding="utf-8") as f:
            readme_content = f.read()
        st.markdown(readme_content)
    except FileNotFoundError:
        st.error("README.md ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

elif app_mode == "ğŸ“š ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°ä¸€è¦§":
    st.sidebar.subheader("ğŸ“š ã‚«ã‚¿ãƒ­ã‚°è¡¨ç¤ºãƒ•ã‚£ãƒ«ã‚¿")
    for label, category_id in CATEGORY_MAPPING.items():
        if st.sidebar.checkbox(label, value=True, key=f"cat_check_{category_id}"):
            selected_categories.append(category_id)

if app_mode == "ğŸ’¬ AIãƒãƒ£ãƒƒãƒˆ (RAG)":
    st.title("ğŸ§¬ NEXT-GEN CAREER BRAIN")
    st.image("tech-trend-rag-family.jpg", caption="Concept: The Future Career Exploring System", use_container_width=True)
    st.markdown("---")
    st.markdown("##### **[ACCESS GRANTED]** KNOWLEDGE SYSTEM READY FOR QUERY.")
    
    # ğŸ”Œ ã‚·ã‚¹ãƒ†ãƒ ãƒ•ãƒ­ãƒ¼å›³ (Mermaidç”»åƒç‰ˆ)
    st.markdown("#### ğŸ”Œ System Architecture")
    render_mermaid("""
    graph LR
        %% ãƒãƒ¼ãƒ‰å®šç¾© (HTMLã‚¿ã‚°é™¤å»)
        User((User/Query))
        DB[(Vector DB)]
        AI[[Gen-AI Claude]]
        Output>Output Result]

        %% ãƒ•ãƒ­ãƒ¼å®šç¾©
        User -->|Search| DB
        DB -->|Retrieval| AI
        User -->|Context| AI
        AI -->|Generation| Output

        %% æ‹¡å¼µæ©Ÿèƒ½ã‚¨ãƒªã‚¢
        subgraph Expansion [Expansion Features]
            direction TB
            DeepDive(Deep Dive Analysis)
            Map(Tech Map Visualization)
            Fun(2035 Vision Card/Diary)
        end
        
        %% æ¥ç¶š
        Output -.->|Analyze| DeepDive
        Output -.->|Visualize| Map
        Output -.->|Imagine| Fun

        %% ã‚¹ã‚¿ã‚¤ãƒ«å®šç¾©
        style User fill:#e8f0fe,stroke:#333,stroke-width:2px
        style DB fill:#e6f3ff,stroke:#00f,stroke-width:2px
        style AI fill:#ffebee,stroke:#f00,stroke-width:2px
        style Output fill:#d4edda,stroke:#333,stroke-width:2px
        style Expansion fill:#fff,stroke:#999,stroke-dasharray: 5 5
    """) 
    st.markdown("---")

    if "rag_result" not in st.session_state: st.session_state.rag_result = None
    if "last_query" not in st.session_state: st.session_state.last_query = ""
    if "thought_expansion" not in st.session_state: st.session_state.thought_expansion = None
    if "career_card" not in st.session_state: st.session_state.career_card = None
    if "future_diary" not in st.session_state: st.session_state.future_diary = None

    query = st.text_area("Enter Your Question ...ğŸ¤£æ—¥æœ¬èªã§ãˆãˆã‚ˆğŸ¤£", height=100)

    if st.button("ğŸ” Research Techs ", type="primary", key='rag_search_button'):
        if not selected_categories:
            st.error("âš ï¸ æ¤œç´¢å¯¾è±¡ã‚½ãƒ¼ã‚¹ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        elif query:
            st.session_state.thought_expansion = None
            st.session_state.career_card = None
            st.session_state.future_diary = None
            with st.spinner("Analyzing 700 Data Feeds... Standby for Analysis."):
                st.session_state.rag_result = run_rag_search(query, selected_categories)
                st.session_state.last_query = query
        else:
            st.error("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    if st.session_state.rag_result:
        result = st.session_state.rag_result
        if isinstance(result, dict):
            st.markdown(f"**ğŸ’¡ å›ç­”**\n\n{result['answer']}")
            st.markdown("---")
            sources_str = ', '.join(result['sources']) if result['sources'] else "ãªã—"
            st.markdown(f"**ğŸ“š å‚ç…§ã•ã‚ŒãŸè³‡æ–™:** {sources_str}") 
            with st.expander("ğŸ“„ å‚ç…§ãƒ‡ãƒ¼ã‚¿ï¼ˆåŸæ–‡ãƒ»æŠ½å‡ºãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼‰ã‚’ç¢ºèªã™ã‚‹"):
                st.caption("â–¼ RAGã§ä½¿ç”¨ã•ã‚ŒãŸåŸæ–‡")
                st.code(result['context'], language="markdown")
                # ğŸš¨ æ€è€ƒã‚¨ãƒ¬ãƒ™ãƒ¼ã‚¿ãƒ¼ç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
                if result.get('meta_context'):
                    st.caption("â–¼ æ€è€ƒã‚¨ãƒ¬ãƒ™ãƒ¼ã‚¿ãƒ¼ç”¨æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ï¼ˆè¦ç´„ãƒ»åˆ†æï¼‰")
                    st.code(result['meta_context'], language="markdown")
            
            st.markdown("---")
            st.subheader("ğŸ’¡ Deep Dive & Expansion")
            c1, c2, c3 = st.columns(3)
            
            meta_context = result.get('meta_context', '')
            
            with c1: 
                if st.button("â¬†ï¸ æŠ½è±¡åŒ–", key="btn_abs", use_container_width=True):
                    with st.spinner("Thinking Macro..."):
                        # ğŸš¨ DBãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã—ã¦åˆ†æ
                        st.session_state.thought_expansion = generate_thought_expansion(
                            st.session_state.last_query, "abstract", meta_context)
            with c2: 
                if st.button("â¬‡ï¸ å…·ä½“åŒ–", key="btn_con", use_container_width=True):
                    with st.spinner("Thinking Micro..."):
                        st.session_state.thought_expansion = generate_thought_expansion(
                            st.session_state.last_query, "concrete", meta_context)
            with c3: 
                if st.button("â†”ï¸ æ¨ªå±•é–‹", key="btn_ana", use_container_width=True):
                    with st.spinner("Connecting Dots..."):
                        st.session_state.thought_expansion = generate_thought_expansion(
                            st.session_state.last_query, "analogous", meta_context)

            if st.session_state.thought_expansion:
                d = st.session_state.thought_expansion
                st.markdown(f"#### {d.get('title', 'Analysis')}")
                st.caption("â€» æ¤œç´¢ã•ã‚ŒãŸæŠ€è¡“è³‡æ–™ã®ã€Œè¦ç´„ãƒ»åˆ†æã€æƒ…å ±ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€AIãŒæ´å¯Ÿã‚’åºƒã’ã¾ã—ãŸã€‚")
                for item in d.get('items', []): st.write(f"â€¢ {item}")

            st.markdown("")
            if st.button("ğŸ•¸ï¸ æŠ€è¡“ä½“ç³»ãƒãƒƒãƒ—ã‚’è¡¨ç¤ºã™ã‚‹", key="btn_map", use_container_width=True):
                with st.spinner("Mapping..."):
                    dot = generate_tech_hierarchy(st.session_state.last_query)
                    if dot:
                        st.success("âœ… ãƒãƒƒãƒ—ç”Ÿæˆå®Œäº†")
                        st.graphviz_chart(dot)
                        st.caption("â€» AIç”Ÿæˆã®æ¦‚å¿µå›³")
                    else:
                        st.error("ãƒãƒƒãƒ—ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")

            st.markdown("---")
            st.subheader("ğŸš€ 2035 Vision Simulation")
            ec1, ec2 = st.columns(2)
            with ec1:
                if st.button("ğŸƒ æœªæ¥ã®ååˆº", key="btn_card", use_container_width=True):
                    with st.spinner("Designing..."):
                        st.session_state.career_card = generate_future_career(st.session_state.last_query)
                        st.session_state.future_diary = None
            with ec2:
                if st.button("ğŸ“– æœªæ¥ã®æ—¥è¨˜", key="btn_diary", use_container_width=True):
                    with st.spinner("Writing..."):
                        st.session_state.future_diary = generate_future_diary(st.session_state.last_query)
                        st.session_state.career_card = None

            if st.session_state.career_card:
                c = st.session_state.career_card
                st.success("âœ… 2035 Career Prediction")
                with st.container(border=True):
                    col_img, col_txt = st.columns([1, 3])
                    with col_img: st.image("https://img.icons8.com/fluency/96/future.png", width=80)
                    with col_txt:
                        st.markdown(f"### {c.get('job_title', 'Unknown Job')}")
                        st.metric(label="æƒ³å®šå¹´å (2035)", value=c.get('estimated_salary', '---'))
                    st.write(f"**Mission:** {c.get('mission', '')}")
                    st.write(f"**Skills:** {', '.join(c.get('required_skills', []))}")

            if st.session_state.future_diary:
                d = st.session_state.future_diary
                st.info("âœ… 2035 Daily Log")
                with st.container(border=True):
                    st.markdown(f"### ğŸ“– {d.get('title', 'Diary')}")
                    st.caption(f"ğŸ“… {d.get('date', '')} | âœï¸ {d.get('author_profile', '')}")
                    st.write(d.get('content', ''))
        else:
            st.error(result)

elif app_mode == "ğŸ“š ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°ä¸€è¦§":
    st.title("ğŸ“š Data Catalog")
    df = get_all_data_as_df()
    if not df.empty:
        df_filtered = df[df['Category'].isin(selected_categories)]
        st.info(f"å…¨ãƒ‡ãƒ¼ã‚¿æ•°: {len(df)} ä»¶ / è¡¨ç¤ºä¸­: {len(df_filtered)} ä»¶")
        st.dataframe(df_filtered, use_container_width=True, hide_index=True)
    else:
        st.warning("ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

# ğŸš¨ ä¿®æ­£: ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã®400ä»¶åˆ¶é™æ©Ÿèƒ½
elif app_mode == "ğŸ•¸ï¸ ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•":
    st.title("ğŸ•¸ï¸ Knowledge Graph")
    st.markdown("é¸æŠã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã®æŠ€è¡“ãƒ¬ãƒãƒ¼ãƒˆï¼ˆé’ï¼‰ã¨ã‚¿ã‚°ï¼ˆèµ¤ï¼‰ã®é–¢é€£æ€§ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚")
    if not selected_categories:
        st.warning("ğŸ‘ˆ å·¦å´ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã€å¯è¦–åŒ–ã—ãŸã„ã‚«ãƒ†ã‚´ãƒªã«ãƒã‚§ãƒƒã‚¯ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚")
    else:
        with st.spinner("ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ä¸­..."):
            nodes, edges = build_knowledge_graph(selected_categories)
            if len(nodes) > 400:
                st.error(f"âš ï¸ ãƒãƒ¼ãƒ‰æ•°ãŒå¤šã™ãã¾ã™ ({len(nodes)}ä»¶)ã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã®ãƒ•ãƒªãƒ¼ã‚ºã‚’é˜²ããŸã‚ã€ã‚«ãƒ†ã‚´ãƒªã‚’çµã£ã¦ãã ã•ã„ã€‚ï¼ˆæ¨å¥¨: 400ä»¶ä»¥ä¸‹ï¼‰")
            elif not nodes:
                st.warning("é¸æŠã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã«ãƒ‡ãƒ¼ã‚¿ï¼ˆã¾ãŸã¯ã‚¿ã‚°ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            else:
                st.info(f"Nodes: {len(nodes)} | Edges: {len(edges)}")
                config = Config(width="100%", height=600, directed=False, physics=True, hierarchical=False, collapsible=True)
                agraph(nodes=nodes, edges=edges, config=config)

st.sidebar.markdown("---")
if st.sidebar.button("ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ", key='logout'):
    st.session_state["password_correct"] = False
    st.session_state["current_user"] = None
    st.session_state.rag_result = None
    st.rerun()
